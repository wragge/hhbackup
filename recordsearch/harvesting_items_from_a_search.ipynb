{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest items from a search in RecordSearch\n",
    "\n",
    "Ever searched for items in RecordSearch and wanted to save the results as a CSV file, or in some other machine-readable format? This notebook walks you through the process of creating, managing, and saving item searches – all the way from search terms to downloadable dataset. You can even download all the images from items that have been digitised!\n",
    "\n",
    "RecordSearch doesn't currently have an option for downloading machine-readable data. So to get collection metadata in a structured form, we have to resort of screen-scraping. All the screen-scraping code used in this notebook is contained in the [recordsearch_tools](https://github.com/wragge/recordsearch_tools) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>If you haven't used one of these notebooks before, they're basically web pages in which you can write, edit, and run live code. They're meant to encourage experimentation, so don't feel nervous. Just try running a few cells and see what happens!</p>\n",
    "\n",
    "<p>\n",
    "    Some tips:\n",
    "    <ul>\n",
    "        <li>Code cells have boxes around them.</li>\n",
    "        <li>To run a code cell click on the cell and then hit <b>Shift+Enter</b>. The <b>Shift+Enter</b> combo will also move you to the next cell, so it's a quick way to work through the notebook.</li>\n",
    "        <li>While a cell is running a <b>*</b> appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number.</li>\n",
    "        <li>In most cases you'll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones.</li>\n",
    "        <li>To edit a code cell, just click on it and type stuff. Remember to run the cell once you've finished editing.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<p><b>Is this thing on?</b> If you can't edit or run any of the code cells, you might be viewing a static (read only) version of this notebook. Click here to <a href=\"https://mybinder.org/v2/gh/GLAM-Workbench/recordsearch/master?urlpath=lab%2Ftree%2Fharvesting_items_from_a_search.ipynb\">load a <b>live</b> version</a> running on Binder.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Import what we need](#1.-Import-what-we-need)\n",
    "2. [Available search parameters](#2.-Available-search-parameters)\n",
    "3. [Create a search](#3.-Create-a-search)\n",
    "4. [Changing how your search results are delivered](#4.-Changing-how-your-search-results-are-delivered)\n",
    "5. [Harvesting a complete set of (less than 20,000) results](#5.-Harvesting-a-complete-set-of-(less-than-20,000)-results)\n",
    "6. [Managing harvests](#6.-Managing-harvests)\n",
    "7. [Saving harvests](#7.-Saving-a-harvest)\n",
    "8. [Harvesting complete series](#8.-Harvesting-complete-series)\n",
    "9. [Saving images from digitised files](#9.-Saving-images-from-digitised-files)\n",
    "10. [Saving image metadata](#10.-Save-images-metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "      padding: 0;\n",
       "      border: 0;\n",
       "    }\n",
       "    </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from tinydb import TinyDB, Query\n",
    "from tinydb.operations import increment\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from recordsearch_tools.client import RSSearchClient, TooManyError\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from IPython.display import display, FileLink, HTML\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from slugify import slugify\n",
    "\n",
    "# Make sure the 'data' directory exists\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# This is a workaround for a problem with tqdm adding space to cells\n",
    "HTML(\"\"\"\n",
    "    <style>\n",
    "    .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
    "      padding: 0;\n",
    "      border: 0;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Available search parameters\n",
    "\n",
    "The available search parameters are the same as those in RecordSearch's Advanced Search form. There's lots of them, but you'll probably only end up using a few like `kw` and `series`. Note that you can use \\* for wildcard searches as you can in the web interface. So setting `kw` to 'wragge\\*' will find both 'wragge' and 'wragges'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `kw` – string containing keywords to search for\n",
    "* `kw_options` – how to interpret `kw`, possible values are:\n",
    "    * 'ALL' – return results containing all of the keywords (default)\n",
    "    * 'ANY' – return results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw` as a phrase rather than a list of words\n",
    "* `kw_exclude` – string containing keywords to exclude from search\n",
    "* `kw_exclude_options` – how to interpret `kw_exclude`, possible values are:\n",
    "    * 'ALL' – exclude results containing all of the keywords (default)\n",
    "    * 'ANY' – exclude results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw_exact` as a phrase rather than a list of words\n",
    "* `search_notes` – set to 'on' to search item notes as well as metadata\n",
    "* `series` – search for items in this series\n",
    "* `series_exclude` – exclude items from this series\n",
    "* `control` – search for items matching this control symbol\n",
    "* `control_exclude` – exclude items matching this control symbol\n",
    "* `barcode` – search for items with this barcode number\n",
    "* `date_from` – search for items with a date (year) greater than or equal to this, eg. '1935'\n",
    "* `date_to` – search for items with a date (year) less than or equal to this\n",
    "* `formats` – limit search to items in a particular format, see possible values below\n",
    "* `formats_exclude` – exclude items in a particular format, see possible values below\n",
    "* `locations` – limit search to items held in a particular location, see possible values below\n",
    "* `locations_exclude` – exclude items held in a particular location, see possible values below\n",
    "* `access` – limit to items with a particular access status, see possible values below\n",
    "* `access_exclude` – exclude items with a particular access status, see possible values below\n",
    "* `digital` – set to 'on' to limit to items that are digitised\n",
    "\n",
    "\n",
    "Possible values for `formats` and `formats_exclude`: \n",
    "\n",
    "* 'Paper files and documents'\n",
    "* 'Index cards'\n",
    "* 'Bound volumes'\n",
    "* 'Cartographic records'\n",
    "* 'Photographs'\n",
    "* 'Microforms'\n",
    "* 'Audio-visual records'\n",
    "* 'Audio records'\n",
    "* 'Electronic records'\n",
    "* '3-dimensional records'\n",
    "* 'Scientific specimens'\n",
    "* 'Textiles'\n",
    "\n",
    "Possible values for `locations` and `locations_exclude`:\n",
    "\n",
    "* 'NAT, ACT'\n",
    "* 'Adelaide'\n",
    "* 'Australian War Memorial'\n",
    "* 'Brisbane'\n",
    "* 'Darwin'\n",
    "* 'Hobart'\n",
    "* 'Melbourne'\n",
    "* 'Perth'\n",
    "* 'Sydney'\n",
    "\n",
    "Possible values for `access` and `access_exclude`:\n",
    "\n",
    "* 'OPEN'\n",
    "* 'OWE'\n",
    "* 'CLOSED'\n",
    "* 'NYE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a search\n",
    "\n",
    "Once you've decided on your parameters you can use them to create a search. For example, if we wanted to find all items that included the word 'wragge' and were digitised, our parameters would be:\n",
    "\n",
    "* `kw='wragge'`\n",
    "* `digital='on'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search client\n",
    "c = RSSearchClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the search parameters to the client and save the results\n",
    "results =  c.search(kw='wragge', digital='on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look to see how many results there are in the complete results set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display total results\n",
    "results['total_results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the search client only gets one page of results (containing 20 items) at a time. You can check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many results do we actually have\n",
    "len(results['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'series': 'A2479',\n",
       " 'control_symbol': '17/1306',\n",
       " 'title': 'The Wragge Estate. Property for sale.',\n",
       " 'access_status': 'Open',\n",
       " 'location': 'Canberra',\n",
       " 'contents_dates': {'date_str': '1917 - 1917',\n",
       "  'start_date': '1917',\n",
       "  'end_date': '1917'},\n",
       " 'digitised_status': True,\n",
       " 'digitised_pages': 4,\n",
       " 'identifier': '149309'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Changing how your search results are delivered\n",
    "\n",
    "There are some additional parameters that affect the way the search results are delivered. We'll use some of these to harvest the complete results set.\n",
    "\n",
    "* `page` – return a specific page of research results\n",
    "* `sort` – return results in a specified order, possible values:\n",
    "    * 1 – series and control symbol\n",
    "    * 3 – title\n",
    "    * 5 – start date\n",
    "    * 7 – digitised items first\n",
    "    * 12 – items with pdfs first\n",
    "    * 9 – barcode\n",
    "    * 11 – audio visual items first\n",
    "* `digitised` – set to `True` (default) or `False` to control whether to include the number of pages in each digitised file (if `True`, extra requests are made to get the info which slows things down a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get the second page of results from the search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results =  c.search(kw='wragge', digital='on', page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item in our result set should be different, because it's coming from the second page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'series': 'A6770',\n",
       " 'control_symbol': 'WRAGGE K C',\n",
       " 'title': 'WRAGGE KEITH CLEMENT : Service Number - B/2680 : Date of birth - 16 Jan 1922 : Place of birth - BRISBANE QLD : Place of enlistment - BRISBANE : Next of Kin - RUPERT',\n",
       " 'access_status': 'Open',\n",
       " 'location': 'Canberra',\n",
       " 'contents_dates': {'date_str': '1939 - 1948',\n",
       "  'start_date': '1939',\n",
       "  'end_date': '1948'},\n",
       " 'digitised_status': True,\n",
       " 'digitised_pages': 2,\n",
       " 'identifier': '4523493'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Harvesting a complete set of (less than 20,000) results\n",
    "\n",
    "Ok, we've learnt how to create a search and get back some data, but only getting the first 20 results is not so useful. What if our search contains hundreds or thousands of items? How do we get them all?\n",
    "\n",
    "To save everything, we have to loop through each page in the result set, saving the results as we go. The functions below do just that.\n",
    "\n",
    "But wait! You might have noticed that RecordSearch only displays results for searches that return fewer than 20,000 items. Because the screen scraper is just extracting details from the RecordSearch web pages, the 20,000 limit applies here as well. If your search has more than 20,000 results, you'll need to narrow it down using additional parameters.\n",
    "\n",
    "The main function below is `harvest_items()`. You just give it any of the search parameters listed above. It will loop through all the pages in the result set, saving the items to a simple JSON database using TinyDB.\n",
    "\n",
    "The database will be created in the `data` directory. It's name will include a timestamp, identifying the time at which the harvest was started. For example `db-items-1567492794.json`. There are more functions for using and managing the db files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_results(client, **kwargs):\n",
    "    '''\n",
    "    Get the total number of results returned by a search.\n",
    "    '''\n",
    "    try:\n",
    "        # Get the first page of results, passing digitised=Flase to speed things up\n",
    "        results = client.search(digitised=False, **kwargs)\n",
    "        \n",
    "        # Get the total number of results\n",
    "        total = results['total_results']\n",
    "        \n",
    "    # Uh oh there are more than 20,000 results\n",
    "    except TooManyError:\n",
    "        print('There are more than 20,000 results.')\n",
    "        total = None\n",
    "    return total\n",
    "\n",
    "def harvest_items(start=1, db_path=None, check_duplicates=False, **kwargs):\n",
    "    '''\n",
    "    Harvest items from a search and save them to a database.\n",
    "    Supply any of the search parameters listed above.\n",
    "    \n",
    "    Set check_duplicates to True if you want to check for possible duplicates (probably not necessary in most cases).\n",
    "    '''\n",
    "    # Initiate the client\n",
    "    client = RSSearchClient()\n",
    "    \n",
    "    # Get the total number of results returned by this search\n",
    "    total_results = get_total_results(client, **kwargs)\n",
    "    \n",
    "    # If the number of results is between 1 and 20,000 we can harvest!\n",
    "    if total_results:\n",
    "        \n",
    "        # Calculate the number of results pages\n",
    "        total_pages = math.ceil(int(total_results) / client.results_per_page)\n",
    "        \n",
    "        # Get the current timestamp to add into the meta\n",
    "        timestamp = int(time.time())\n",
    "        \n",
    "        # We're creating a new db\n",
    "        if not db_path:\n",
    "            \n",
    "            # Use the timestamp to create a directory & file name for the db\n",
    "            harvest_path = Path('harvests', str(timestamp))\n",
    "            harvest_path.mkdir(parents=True, exist_ok=True)\n",
    "            db_path = Path(harvest_path, 'db-items.json')\n",
    "            \n",
    "            # Create the new db\n",
    "            db = TinyDB(db_path)\n",
    "            \n",
    "        else:\n",
    "            # If we have an existing db, open it\n",
    "            db = TinyDB(db_path)\n",
    "            \n",
    "        # Save the details of this harvest (or restart) to the 'meta' table of the database\n",
    "        # This keeps the query metadata with the results, and helps us to restart the harvest if necessary.\n",
    "        db.table('meta').insert({\n",
    "            'timestamp': timestamp, \n",
    "            'total_results': int(total_results), \n",
    "            'total_pages': total_pages, \n",
    "            'start_page': start,\n",
    "            'pages_harvested': start - 1,\n",
    "            'results_per_page': client.results_per_page,\n",
    "            'params': kwargs\n",
    "        })\n",
    "            \n",
    "        # Loop through the range of pages\n",
    "        for page in tqdm(range(start, total_pages + 1), unit='page', desc='Pages:'):\n",
    "            \n",
    "            # Get results from each page\n",
    "            # Note that sort is set to 9 (barcode) to make sure the pages stay in the same order\n",
    "            # If we don't set the sort param we can end up getting duplicates and missing records\n",
    "            items = client.search(sort=9, page=page, **kwargs)\n",
    "            \n",
    "            # If check_duplicates is set to true we'll loop through each result individually\n",
    "            # This will slow things down a little, and is probably uncessary unless you're stitching together multiple harvests\n",
    "            if check_duplicates is True:\n",
    "                Record = Query()\n",
    "                \n",
    "                # Loop through results\n",
    "                for item in items['results']:\n",
    "                    \n",
    "                    # If they're not in the db, then add them!\n",
    "                    if db.table('items').contains(Record.identifier == item['identifier']) is False:\n",
    "                        db.table('items').insert(item)\n",
    "            else:\n",
    "                # Save the results from this page to the db in one hit\n",
    "                db.table('items').insert_multiple(items['results'])\n",
    "            Meta = Query()\n",
    "            db.table('meta').update(increment('pages_harvested'), Meta.timestamp == timestamp)\n",
    "            # Pause briefly\n",
    "            time.sleep(0.5)\n",
    "    return db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a harvest!\n",
    "harvest_items(series='J3115')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Managing harvests\n",
    "\n",
    "If you're doing a large harvest, you might find that it fails part way through. You might also want to check on the details of a past harvest, or even reharvest a query to see if anything new has been added. Because we've saved the harvest metadata and results into a TinyDB database, it's easy to perform some basic checks and management tasks.\n",
    "\n",
    "There are three main functions defined below:\n",
    "\n",
    "* `harvest_report()` – prints basic details of a harvest\n",
    "* `harvest_restart()` – restarts a failed harvest\n",
    "* `reharvest_items()` – creates a new harvest using the query settings of an existing harvest\n",
    "\n",
    "In each case you can specify the path to an existing harvest database, something like `data/db-items-1567480717.json`. If you don't specify a database, the function will assume you want the most recent.\n",
    "\n",
    "Here's an example of the output from `harvest_report()`.\n",
    "\n",
    "```\n",
    "Harvest started: 2019-09-03 15:21:11\n",
    "Items harvested: 200 of 200\n",
    "\n",
    "{'timestamp': 1567488071,\n",
    " 'total_results': 200,\n",
    " 'total_pages': 10,\n",
    " 'results_per_page': 20,\n",
    " 'params': {'kw': 'wragge'}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_db():\n",
    "    '''\n",
    "    Get the database created by the most recent harvest.\n",
    "    '''\n",
    "    p = Path('harvests')\n",
    "    harvests = sorted([d for d in p.iterdir() if d.is_dir()])\n",
    "    try:\n",
    "        latest = Path(harvests[-1], 'db-items.json')\n",
    "    except IndexError:\n",
    "        print('No databases')\n",
    "        latest = None\n",
    "    return latest\n",
    "\n",
    "def get_db(db_path):\n",
    "    '''\n",
    "    Get a harvest database.\n",
    "    If db_path is supplied then return that db.\n",
    "    If not, then return the most recently created db and path.\n",
    "    '''\n",
    "    db = None\n",
    "    if not db_path:\n",
    "        db_path = get_latest_db()\n",
    "    db = TinyDB(db_path)\n",
    "    return (db_path, db)\n",
    "\n",
    "def harvest_report(db_path=None):\n",
    "    '''\n",
    "    Print a report of the specified harvest.\n",
    "    If db_path is not supplied, display details from the most recently created harvest.\n",
    "    '''\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[0]\n",
    "        date = datetime.fromtimestamp(meta['timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        items_harvested = len(db.table('items').all())\n",
    "        print(f'Harvest started: {date}')\n",
    "        print(f'Items harvested: {items_harvested} of {meta[\"total_results\"]}\\n')\n",
    "        display(meta)\n",
    "    \n",
    "def harvest_restart(db_path=None):\n",
    "    '''\n",
    "    Attempt to restart the specified harvest.\n",
    "    If db_path is not supplied, restart the most recently created harvest.\n",
    "    '''\n",
    "    db_path, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[-1]\n",
    "        pages_harvested = meta['pages_harvested']\n",
    "        if pages_harvested < meta['total_pages']:\n",
    "            start = pages_harvested + 1\n",
    "            harvest_items(db_path=str(db_path), start=start, **meta['params'])\n",
    "        else:\n",
    "            print('Harvest complete')\n",
    "            \n",
    "def reharvest_items(db_path=None):\n",
    "    '''\n",
    "    Harvest items using the parameters of the specified db.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[0]\n",
    "        harvest_items(**meta['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display details of the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: harvest_report('data/db-items-1567480968.json')\n",
    "harvest_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: harvest_restart('data/db-items-1567480968.json')\n",
    "harvest_restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new harvest using the parameters of the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: reharvest_items('data/db-items-1567480968.json')\n",
    "reharvest_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving a harvest\n",
    "\n",
    "Although your harvest is already saved in a TinyDB database, you might want to convert it to a simpler format for download and analysis. The functions below provide two options:\n",
    "\n",
    "* `save_harvest_as_json()` – save the harvested items as a JSON file\n",
    "* `save_harvest_as_csv()` – save the harvested items as a CSV file\n",
    "\n",
    "The columns in the CSV-formatted file are:\n",
    "\n",
    "* `identifier` – the barcode number\n",
    "* `series` – identifier of the series which contains the item\n",
    "* `control_symbol` – individual control symbol\n",
    "* `title` – title of the item\n",
    "* `contents_date_str` – the contents date string as in RecordSearch\n",
    "* `contents_start_date` – the first date in the contents date string converted to ISO format\n",
    "* `contents_end_date` – the second date in the contents date string converted to ISO format\n",
    "* `location` – where the file is held\n",
    "* `access_status` – 'Closed', 'Open', 'OWE, or 'NYE\n",
    "* `digitised_status` – True/False, has the file been digitised\n",
    "* `digitised_pages` – number of pages in the digitised file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_harvest_as_json(db_path=None):\n",
    "    '''\n",
    "    Save harvested items as a json file.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get harvest metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Get harvested items\n",
    "        items = db.table('items').all()\n",
    "        \n",
    "        # Set file name and path\n",
    "        filename = Path(f'harvests/{meta[\"timestamp\"]}/items.json')\n",
    "        \n",
    "        # Dump items to a JSON file\n",
    "        with open(filename, 'w') as json_file:\n",
    "            json.dump(items, json_file)\n",
    "        \n",
    "        # Display link to download\n",
    "        display(FileLink(filename))\n",
    "            \n",
    "def save_harvest_as_csv(db_path=None):\n",
    "    '''\n",
    "    Save harvested items as a CSV file.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get harvest metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Get harvested items\n",
    "        items = db.table('items').all()\n",
    "        \n",
    "        # Flatten the date field using json_normalise and convert the items to a dataframe\n",
    "        df = pd.DataFrame(pd.json_normalize(items))\n",
    "        \n",
    "        # Rename the date columns\n",
    "        df.rename(columns={'contents_dates.date_str': 'contents_date_str', 'contents_dates.start_date': 'contents_start_date', 'contents_dates.end_date': 'contents_end_date'}, inplace=True)\n",
    "        \n",
    "        # Put the columns in a nice order\n",
    "        df = df[['identifier', 'series', 'control_symbol', 'title', 'contents_date_str', 'contents_start_date', 'contents_end_date', 'location', 'access_status', 'digitised_status', 'digitised_pages']]\n",
    "        \n",
    "        # Set file name and path\n",
    "        filename = Path(f'harvests/{meta[\"timestamp\"]}/items.csv')\n",
    "        \n",
    "        # Save as CSV\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Display link to download\n",
    "        display(FileLink(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the most recent harvest as a json file\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('data/db-1567480968.json')\n",
    "save_harvest_as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the most recent harvest as a CSV file\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('data/db-1567480968.json')\n",
    "save_harvest_as_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Harvesting complete series\n",
    "\n",
    "You can harvest a complete series using the examples above. Just set the `series` search parameter to the series identifier. For example:\n",
    "\n",
    "``` python\n",
    "harvest_items(series='A6119')\n",
    "```\n",
    "But what do you do if the series contains more than 20,000 items? One way of splitting the series up into harvestable chunks is to use wildcard values and the `control` search parameter. For example, B13 has more than 20,000 items, but if we limit the results to items with a control symbol starting with '1', we bring the number down to under 20,000:\n",
    "\n",
    "``` python\n",
    "harvest_items(series='B13', control='1*')\n",
    "```\n",
    "\n",
    "To make sure we get everything in the series we can repeat the harvest using a range of prefixes for the control symbol – the easiest approach is simply to loop through each letter and number from A to Z and 0 to 9. That's exactly what the `harvest_large_series()` function below does. The key thing is the `control_range`. By default, it is a list that looks like this:\n",
    "\n",
    "``` python\n",
    "['A*', 'B*', 'C*', 'D*', 'E*', 'F*', 'G*', 'H*', 'I*', 'J*', 'K*', 'L*', 'M*', 'N*', 'O*', 'P*', 'Q*', 'R*', 'S*', 'T*', 'U*', 'V*', 'W*', 'X*', 'Y*', 'Z*', '0*', '1*', '2*', '3*', '4*', '5*', '6*', '7*', '8*', '9*']\n",
    "```\n",
    "\n",
    "Note that it's possible to get duplicate items this way because some items include earlier versions of control symbols and these are searched as well as the current ones. We can filter out the duplicates by asking the harvest function to check whether an item is already in the db before saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_large_series(series, control_range=None, restart=False, db_path=None):\n",
    "    '''\n",
    "    RecordSearch will not return more than 20,000 results.\n",
    "    If a series has more than 20,000 items you'll need to break it up.\n",
    "    The easiest way to do this is to add a param for control_symbol.\n",
    "    This function will break break a series harvest down into a series of harvests --\n",
    "    using each letter and number with a wildcard as the control_symbol parameter.\n",
    "    This should be enough to harvest most large series, but in some cases you might need to supply a custom list of control_symbol prefixes.\n",
    "    '''\n",
    "    start = 1\n",
    "    \n",
    "    # If you don't supply a range of prefixes, use A-Z and 0-9\n",
    "    if not control_range:\n",
    "        control_range = [letter + '*' for letter in string.ascii_uppercase] + [str(number) + '*' for number in range(0, 10)]\n",
    "        \n",
    "    # Try to restart a failed harvest\n",
    "    if restart is True:\n",
    "        \n",
    "        # Get the current db\n",
    "        db_path, db = get_db(db_path)\n",
    "        \n",
    "        # Get the most recent metadata\n",
    "        meta = db.table('meta').all()[-1]\n",
    "        \n",
    "        # Get the last used control\n",
    "        control = meta['params']['control']\n",
    "        \n",
    "        # Limit the prefixes to ones that haven't been used yet\n",
    "        control_range = control_range[control_range.index(control):]\n",
    "        \n",
    "        # Work out which page to start from\n",
    "        pages_harvested = meta['pages_harvested']\n",
    "        if pages_harvested < meta['total_pages']:\n",
    "            start = pages_harvested + 1\n",
    "    \n",
    "    # Loop through control prefixes initiating a new harvest for each\n",
    "    # After the first harvest we'll have a value for dp_path, which we can then use in subsequent calls.\n",
    "    # This means all the separate harvests will be saved in a single db\n",
    "    for control in control_range:\n",
    "        \n",
    "        # Start a new harvest using the control symbol. Set harvester to check for duplicates.\n",
    "        db_path = harvest_items(db_path=db_path, start=start, check_duplicates=True, series=series, control=control)\n",
    "        start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvest a series with more than 20,000 items\n",
    "harvest_large_series(series='B13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some series that can't easily be sliced up into chunks of less than 20,000. Either they're extremely large, or their range of control symbol prefixes is very small. In this case you have to experiment to find a list of prefixes that will work. How do you know? The function below let's you test a range of control symbol prefixes to check whether the slices return fewer than 20,000 items. If you give it a series identifier, it will use the default range of prefixes described above. But you can also feed it a customised list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_control_prefixes(series, control_range=None):\n",
    "    '''\n",
    "    Test a range of control symbol prefixes to see if they split a series up into chunks of less than 20,000 items.\n",
    "    Prints the number of results for each prefix, or 'More than 20,000' if greater than 20,000.\n",
    "    '''\n",
    "    c = RSSearchClient()\n",
    "    if not control_range:\n",
    "        control_range = [letter + '*' for letter in string.ascii_uppercase] + [str(number) + '*' for number in range(0, 10)]\n",
    "    for control in control_range:\n",
    "        try:\n",
    "            results =  c.search(series=series, control=control)\n",
    "            total = results['total_results']\n",
    "        except TooManyError:\n",
    "            total = 'More than 20,000'\n",
    "        print(f'{control}: {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, series A1 has more than 60,000 items and uses the year as the control symbol prefix. In this case we can create a list of control symbol prefixes to slice the series up by year-like combinations of digits. The following cell  creates such a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For series like A1 that use the year as the control symbol prefix, this range should work.\n",
    "control_range = [str(num) + '*' for num in range(2,10)] + ['1{}*'.format(num2) for num2 in [str(num) for num in range(0,9)]] + ['19{}*'.format(num2) for num2 in [str(num) for num in range(1,10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cell above is run, it creates a list that looks like this:\n",
    "    \n",
    "``` python\n",
    "['2*', '3*', '4*', '5*', '6*', '7*', '8*', '9*', '10*', '11*', '12*', '13*', '14*', '15*', '16*', '17*', '18*', '191*', '192*', '193*', '194*', '195*', '196*', '197*', '198*', '199*']\n",
    "```\n",
    "\n",
    "Note how it splits years in the 20th century by decade? To test that this will actually work, we can feed this list to the `test_control_prefixes()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_control_prefixes('A1', control_range=control_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_control_prefixes()` prints out the following results:\n",
    "\n",
    "```\n",
    "2*: 0\n",
    "3*: 2\n",
    "4*: 0\n",
    "5*: 0\n",
    "6*: 0\n",
    "7*: 0\n",
    "8*: 0\n",
    "9*: 2\n",
    "10*: 1\n",
    "11*: 0\n",
    "12*: 0\n",
    "13*: 0\n",
    "14*: 0\n",
    "15*: 0\n",
    "16*: 0\n",
    "17*: 1\n",
    "18*: 0\n",
    "191*: 17501\n",
    "192*: 18908\n",
    "193*: 18247\n",
    "194*: 0\n",
    "195*: 0\n",
    "196*: 0\n",
    "197*: 0\n",
    "198*: 0\n",
    "199*: 2\n",
    "```\n",
    "\n",
    "Note that all the slices are less than 20,000. Yay it works! You might also note that while most of the control symbol prefixes are years, there are a few oddities as well. In general, you can't rely on control symbols being consistent. Even if the series notes say they use numbers, you need to check to make sure no letters have snuck in. So basically it takes so it takes a fair bit of trial and error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined a custom control range as above, we can feed it to `harvest_large_series()` to use in place of the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom range to harvest a large series\n",
    "harvest_large_series(series='A1', control_range=control_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving images from digitised files\n",
    "\n",
    "Once you've saved all the metadata from your search, you can use it to download images from all the items that have been digitised.\n",
    "\n",
    "The function below will look for all items that have a `digitised_status` of True in a harvest db, and then download all the images from them. The images will be saved in a folder named with the timestamp of the original harvest, eg. `data/items-1567492815-images`.\n",
    "\n",
    "Within that folder, the images from each item will be saved in a separate folder, named using the `series`, `control_symbol`, and `identifier` values. So the folder `a2487-1919-8962-156686` contains images from a file in series A2487, with the control symbol 1919/8962, and the barcode of 156686. Images are named with the `identifier` (barcode) and page numbers, eg: `156686-1.jpg`.\n",
    "\n",
    "The function also saves some image metadata back into the harvest db, specifically:\n",
    "\n",
    "* `image_dir` – the path where the image has been saved\n",
    "* `image_name` – the filename of the image\n",
    "* `identifier` – the item barcode\n",
    "* `page` – the page number\n",
    "* `width` – the width of the image (in pixels)\n",
    "* `height` – the height of the image (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_images(db_path=None):\n",
    "    '''\n",
    "    Download images from all the digitised files in a harvest.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    \n",
    "    # Get the harvest db\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get the metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Find items where digitised_status is True\n",
    "        Record = Query()\n",
    "        items = db.table('items').search(Record.digitised_status == True)\n",
    "        \n",
    "        # Loop through digitised items\n",
    "        for item in tqdm(items, desc='Items'):\n",
    "            \n",
    "            # Set name of the folder in which to save the images\n",
    "            image_dir = Path(f'harvests/{meta[\"timestamp\"]}/images/{slugify(item[\"series\"])}-{slugify(item[\"control_symbol\"])}-{item[\"identifier\"]}')\n",
    "            \n",
    "            # Create the folder (and parent if necessary)\n",
    "            image_dir.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            # Loop through the page numbers\n",
    "            for page in tqdm(range(1, item['digitised_pages'] + 1), desc='Pages', leave=False):\n",
    "                \n",
    "                # Define the image filename using the barcode and page number\n",
    "                filename = Path(f'{image_dir}/{item[\"identifier\"]}-{page}.jpg')\n",
    "                \n",
    "                # Check to see if the image already exists (useful if rerunning a failed harvest)\n",
    "                if not filename.exists():\n",
    "                    \n",
    "                    # If it doens't already exist then download it\n",
    "                    img_url = f'http://recordsearch.naa.gov.au/NaaMedia/ShowImage.asp?B={item[\"identifier\"]}&S={page}&T=P'\n",
    "                    response = requests.get(img_url)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    # Try opening the file as an image\n",
    "                    try:\n",
    "                        image = Image.open(BytesIO(response.content))\n",
    "                    except IOError:\n",
    "                        print('Not an image')\n",
    "                    else:\n",
    "                        # If it's an image, get its dimensions\n",
    "                        width, height = image.size\n",
    "                        \n",
    "                        # Save the image\n",
    "                        image.save(filename)\n",
    "                        \n",
    "                        # Create image metadata\n",
    "                        image_meta = {\n",
    "                            'image_dir': str(image_dir),\n",
    "                            'image_name': '{}-{}.jpg'.format(item['identifier'], page),\n",
    "                            'identifier': item['identifier'],\n",
    "                            'page': page,\n",
    "                            'width': width,\n",
    "                            'height': height\n",
    "                        }\n",
    "                        \n",
    "                        # Add/update the image metadata\n",
    "                        db.table('images').upsert(image_meta, Record.image_name == image_meta['image_name'])\n",
    "                        \n",
    "                        # Pause\n",
    "                        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('harvests/1567480968/db-items.json')\n",
    "harvest_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save images metadata\n",
    "\n",
    "It might be handy to have all the image metadata in a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_as_csv(db_path=None):\n",
    "    '''\n",
    "    Save metadata of harvested images as a CSV file.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    _, db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get harvest metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Get harvested images\n",
    "        images = db.table('images').all()\n",
    "        \n",
    "        # Convert the image metadata to a dataframe\n",
    "        df = pd.DataFrame(images)\n",
    "        \n",
    "        # Set file name and path\n",
    "        filename = Path(f'harvests/{meta[\"timestamp\"]}/images.csv')\n",
    "        \n",
    "        # Save as CSV\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Display link to download\n",
    "        display(FileLink(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image metadata and display a link\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('data/db-1567480968.json')\n",
    "save_metadata_as_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Coming soon, notebooks to help you explore your harvested data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) as part of the [GLAM Workbench](https://glam-workbench.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
