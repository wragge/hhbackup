{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest files with the access status of 'closed'\n",
    "\n",
    "The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. You can read about my efforts to extract and interpret this data in [Inside Story](http://insidestory.org.au/withheld-pending-advice/).\n",
    "\n",
    "While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, FileLink\n",
    "from tinydb import TinyDB, Query\n",
    "from recordsearch_tools.client import RSSearchClient, RSItemClient\n",
    "from recordsearch_tools.utilities import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to match against the reasons in RS to normalise them\n",
    "EXCEPTIONS = [\n",
    "    ['33(1)(a)', r'33\\(1\\)\\(a\\)'],\n",
    "    ['33(1)(b)', r'33\\(1\\)[a\\(\\)]*\\(b\\)'],\n",
    "    ['33(1)(c)', r'33\\(1\\)[ab\\(\\)]*\\(c\\)'],\n",
    "    ['33(1)(d)', r'33\\(1\\)[abc\\(\\)]*\\(d\\)'],\n",
    "    ['33(1)(e)(i)', r'33\\(1\\)[abcd\\(\\)]*\\(e\\)\\(i\\)'],\n",
    "    ['33(1)(e)(ii)', r'33\\(1\\)[abcd\\(\\)]*\\(e\\)\\(ii\\)'],\n",
    "    ['33(1)(e)(iii)', r'33\\(1\\)[abcd\\(\\)]*\\(e\\)\\(iii\\)'],\n",
    "    ['33(1)(f)(i)', r'33\\(1\\)[abcdei\\(\\)]*\\(f\\)\\(i\\)'],\n",
    "    ['33(1)(f)(ii)', r'33\\(1\\)[abcdei\\(\\)]*\\(f\\)\\(ii\\)'],\n",
    "    ['33(1)(f)(iii)', r'33\\(1\\)[abcdei\\(\\)]*\\(f\\)\\(iii\\)'],\n",
    "    ['33(1)(g)', r'33\\(1\\)[abcdefi\\(\\)]*\\(g\\)*'],\n",
    "    ['33(1)(h)', r'33\\(1\\)[abcdefgi\\(\\)]*\\(h\\)'],\n",
    "    ['33(1)(j)', r'33\\(1\\)[abcdefghi\\(\\)]*\\(j\\)'],\n",
    "    ['33(2)(a)', r'33\\(2\\)\\(a\\)'],\n",
    "    ['33(2)(b)', r'33\\(2\\)[a\\(\\)]*\\(b\\)'],\n",
    "    ['33(3)(a)(i)', r'33\\(3\\)\\(a\\)\\(i\\)'],\n",
    "    ['33(3)(a)(ii)', r'33\\(3\\)\\(a\\)\\(ii\\)'],\n",
    "    ['33(3)(b)', r'33\\(3\\)[ai\\(\\) &]*\\(b\\)'],\n",
    "    ['Closed period', r'Closed period.*']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchHarvester():\n",
    "    \"\"\"\n",
    "    Harvest the details of 'Closed' files from RecordSearch.\n",
    "    Saves to a TinyDB database.\n",
    "    harvester = SearchHarvester()\n",
    "    harvester.start_harvest()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.total_pages = None\n",
    "        self.client = RSSearchClient()\n",
    "        self.prepare_harvest(access='Closed')\n",
    "        self.db = TinyDB('data/db-closed2.json')\n",
    "    \n",
    "    @retry(ConnectionError, tries=20, delay=10, backoff=1)\n",
    "    def prepare_harvest(self, **kwargs):\n",
    "        self.client.search(**kwargs)\n",
    "        total_results = self.client.total_results\n",
    "        print('{} items'.format(total_results))\n",
    "        self.total_pages = math.floor(int(total_results) / self.client.results_per_page) + 1\n",
    "        print('{} pages'.format(self.total_pages))\n",
    "    \n",
    "    @retry(ConnectionError, tries=20, delay=10, backoff=1)\n",
    "    def process_item(self, result):\n",
    "        item_client = RSItemClient()\n",
    "        # Search results don't include all the details, so get the full item record\n",
    "        item = item_client.get_summary(entity_id=result['identifier'], date_format='iso')\n",
    "        item['reasons'] = []\n",
    "        # The access reason field can munge together mutiple reasons, so we need to separate & normalise\n",
    "        for reason in item['access_reason']:\n",
    "            matched = False\n",
    "            # Loop through the regexp patterns to see what we can find in the access reason field, save any matches\n",
    "            for exception, pattern in EXCEPTIONS:\n",
    "                if re.match(pattern, reason['reason']):\n",
    "                    item['reasons'].append(exception)\n",
    "                    matched = True\n",
    "            if not matched:\n",
    "                # If nothing matches, just save the original\n",
    "                item['reasons'].append(reason['reason'])\n",
    "        return item\n",
    "    \n",
    "    @retry(ConnectionError, tries=20, delay=10, backoff=1)\n",
    "    def start_harvest(self, start=1):\n",
    "        Record = Query()\n",
    "        for page in tqdm(range(start, self.total_pages + 1), unit='page', desc='Pages:'):\n",
    "            response = self.client.search(access='Closed', page=page, sort='9')\n",
    "            for result in tqdm(response['results'], unit='items', desc='Items on page:', leave=False):\n",
    "                # Save some time by ignoring records we've already harvested\n",
    "                # Handy if you're restarting a failed harvest\n",
    "                if not self.db.contains(Record.identifier == result['identifier']):\n",
    "                    item = self.process_item(result)\n",
    "                    self.db.table('items').upsert(item, Record.identifier == item['identifier'])\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the harvest (may take a few hours)\n",
    "harvester = SearchHarvester()\n",
    "harvester.start_harvest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Load the data from TinyDB\n",
    "    db = TinyDB('data/db-closed.json', default_table='items')\n",
    "    return db.all()\n",
    "\n",
    "def make_filename():\n",
    "    filename = 'data/closed-{}'.format(datetime.datetime.now().strftime('%Y%m%d'))\n",
    "    return filename\n",
    "\n",
    "def save_csv():\n",
    "    items = get_data()\n",
    "    # Flatten the date fields using json_normalise and convert to a dataframe\n",
    "    df = pd.DataFrame(json_normalize(items))\n",
    "    # Rename the dates columns\n",
    "    df.rename(columns={'access_decision.date_str': 'access_decision_date_str', 'access_decision.start_date': 'access_decision_date', 'contents_dates.date_str': 'contents_date_str', 'contents_dates.start_date': 'contents_start_date', 'contents_dates.end_date': 'contents_end_date'}, inplace=True)\n",
    "    # Get the columns we want and put them in a nice order\n",
    "    df = df[['identifier', 'series', 'control_symbol', 'title', 'contents_date_str', 'contents_start_date', 'contents_end_date', 'location', 'access_status', 'access_decision_date_str', 'access_decision_date', 'reasons']]\n",
    "    # Save the reasons lists as | separated strings\n",
    "    df2 = df.copy()\n",
    "    df2['reasons'] = df['reasons'].str.join('|')\n",
    "    filename = '{}.csv'.format(make_filename())\n",
    "    df2.to_csv(filename, index=False)\n",
    "    display(FileLink(filename))\n",
    "    \n",
    "def save_json():\n",
    "    items = get_data()\n",
    "    filename = '{}.json'.format(make_filename())\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(items, json_file)\n",
    "    display(FileLink(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/closed-20200101.csv' target='_blank'>data/closed-20200101.csv</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/recordsearch/notebooks/data/closed-20200101.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='data/closed-20200101.json' target='_blank'>data/closed-20200101.json</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/recordsearch/notebooks/data/closed-20200101.json"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save results as CSV and JSON and provide handy download links\n",
    "save_csv()\n",
    "save_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) as part of the [GLAM Workbench](https://glam-workbench.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
